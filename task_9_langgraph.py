# -*- coding: utf-8 -*-
"""Task-9 LangGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lpsAtiuQLxOAznnIjaCAPNAjPZUXqOQf
"""

!pip install langgraph --quiet

!pip install sentence-transformers --quiet

!pip install langgraph

!pip install -U langchain langchain-community

!pip install faiss-cpu --quiet

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.text_splitter import CharacterTextSplitter

# Your symbolic KB
kb_facts = """\
animal(dog).
animal(cat).
animal(eagle).
animal(sparrow).
flies(eagle).
flies(sparrow).
has_tail(dog).
has_tail(cat).
"""

kb_rules = """\
bird(X) :- animal(X), flies(X).
mammal(X) :- animal(X), has_tail(X).
"""

# Convert to LangChain Documents
docs = [
    Document(page_content=kb_facts, metadata={"source": "facts"}),
    Document(page_content=kb_rules, metadata={"source": "rules"})
]

# Split and embed
splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
kb_chunks = splitter.split_documents(docs)
embedding = HuggingFaceEmbeddings()
vectorstore = FAISS.from_documents(kb_chunks, embedding)
retriever = vectorstore.as_retriever()

# Step 4: Define schema and construct the LangGraph
from langgraph.graph import StateGraph
from typing import TypedDict

# Define the graph schema
class GraphState(TypedDict):
    query: str
    context: str

# Define the retrieve node
def retrieve_node(state: GraphState) -> GraphState:
    docs = retriever.invoke(state["query"])
    context = "\n".join([doc.page_content for doc in docs])
    return {"query": state["query"], "context": context}

# Define the judge node (placeholder for CoT/refinement logic)

# def judge_node(state: GraphState) -> GraphState:
#     print("Final context retrieved for query:")
#     print(state["context"])
#     return state

def judge_node(state: GraphState) -> GraphState:
    query = state["query"]
    context = state["context"]

    # Simulate Chain of Thought by evaluating the relevance
    print("\n Chain of Thought Reasoning:")
    print(f"Query: {query}")
    print("Step 1: Examine retrieved context...")
    print(context)

    # Very simple reasoning example (could be replaced with LLM later)
    if "sparrow" in query.lower() and "flies(sparrow)." in context:
        print("→ Reasoning: The fact 'flies(sparrow)' exists, so yes, it can fly.")
    elif "dog" in query.lower() and "flies(dog)." not in context:
        print("→ Reasoning: No evidence found that dogs fly, so probably not.")
    else:
        print("→ Reasoning: Not enough information to answer definitively.")

    return state


# Build the graph
graph = StateGraph(GraphState)
graph.add_node("retrieve", retrieve_node)
graph.add_node("judge", judge_node)

graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "judge")
graph.set_finish_point("judge")

# Compile the graph
runnable = graph.compile()

output = runnable.invoke({"query": "Can a sparrow fly?"})